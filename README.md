# Data-Lake
Project Data Lake as part of Udacity's Data Engineering Nanodegree

## Purpose of this project

As the startup sparkify is scaling up quickly, their existing data warehouse can not handle the massive data resources efficiently any longer. They heard about Spark and were curious how it could help them. With this project, they can now analyse their data in a distributed way in-memory. This leads to huge speedups comparing to the existing approach and allows them to keep track of their clients' behavior easily.

Moreover, this data lake automates the entire data fusion process combining multiple data sources from AWS S3 into structured data. Also, the structured data gets then stored on AWS S3 again, so Sparkify can use it for further analysis.

## Used input data

The input data contains two S3 buckets:
* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`

The song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```json
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

The log dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

And below is an example of what the data in a log file, 2018-11-12-events.json, looks like.

![2018-11-12-events.json structure](https://video.udacity-data.com/topher/2019/February/5c6c3f0a_log-data/log-data.png)

## Generated tables

| name | type | description | columns |
| ---- | ---- | ----------- | ------- |
| songplays | fact table | records in log data associated with song plays i.e. records with page NextSong | songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent |
| users | dimension table | users in the app | user_id, first_name, last_name, gender, level |
| songs | dimension table | songs in music database | song_id, title, artist_id, year, duration |
| artists | dimension table |  artists in music database | artist_id, name, location, lattitude, longitude |
| time | dimension table | timestamps of records in songplays broken down into specific units | start_time, hour, day, week, month, year, weekday |

## ETL pipeline

The ETL pipeline (see `etl.py`) loads the S3 data sources into Spark dataframes, aggregrates and transforms the data into the described schema and writes the data back to S3 in the parquet format.

## Instructions

1. Create an AWS IAM role with S3 read and write access.
2. Enter the IAM's credentials in the `dl.cfg` configuration file.
3. Create an S3 bucket (note that the zone eu-central-1 may cause issues) and enter the URL to the bucket in `etl.py` as the value of output_data.
4. Run `python3 etl.py` to process the data and store it on your created S3 bucket.
